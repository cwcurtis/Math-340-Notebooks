{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse.linalg import eigsh\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "So, linear algebra is largely concerned with finding solutions to the problem \n",
    "\n",
    "$$\n",
    "A {\\bf x} = {\\bf b}\n",
    "$$\n",
    "\n",
    "where the _vector_ ${\\bf x}$ is an $n$-dimensional list of real values, so that \n",
    "\n",
    "$$\n",
    "{\\bf x} = \\begin{pmatrix} x_{1}\\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix}, ~ {\\bf x} \\in \\mathbb{R}^{n},\n",
    "$$\n",
    "\n",
    "and likewise ${\\bf b} \\in \\mathbb{R}^{m}$ and $A$ is an $m\\times n$ matrix of real values so that\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} A_{11} & A_{12} & \\cdots & A_{1n} \\\\ A_{21} & A_{22} & \\cdots & A_{2n}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{m1} & A_{m2} & \\cdots & A_{mn} \\end{pmatrix},~ A:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^{m}\n",
    "$$\n",
    "\n",
    "### Solvability for Square Matrices\n",
    "\n",
    "* Note, for ${\\bf x},{\\bf y} \\in \\mathbb{R}^{n}$, we define the _inner-product_  $\\left<{\\bf x},{\\bf y}\\right>$ to be\n",
    "$$\n",
    "\\left<{\\bf x},{\\bf y}\\right> = {\\bf y}^{T}{\\bf x} = \\sum_{j=1}^{n}y_{j}x_{j}.\n",
    "$$\n",
    "Show $\\left<{\\bf x},{\\bf y}\\right> = \\left<{\\bf y},{\\bf x}\\right>$, and for a square matrix $A$ that $\\left<A{\\bf x},{\\bf y}\\right> = \\left<{\\bf x},A^{T}{\\bf y}\\right>$.     \n",
    "\n",
    "We now suppose that in our $A{\\bf x} = {\\bf b}$ problem that $A$ is a $n\\times n$ square matrix of real values.  In general we have three outcomes to the question of whether a solution ${\\bf x}$ exists to this problem\n",
    "* There exists ${\\bf y}$ such that $A^{T}{\\bf y}=0$ but $\\left<{\\bf b},{\\bf y} \\right>\\neq 0$.  Then **no** solution ${\\bf x}$ exists to the problem.  \n",
    "* $\\left<{\\bf b},{\\bf y} \\right> = 0$ for all ${\\bf y}\\neq 0$ and $A^{T}{\\bf y} = 0$.  Then an **infinite** number of solutions exists of the form \n",
    "$$\n",
    "{\\bf x} = {\\bf x}_{h} + {\\bf x}_{p}, \n",
    "$$ \n",
    "where $A{\\bf x}_{h} = 0$ and $A{\\bf x}_{p}={\\bf b}$.  Note, we have an infinite number of solutions since for any constant $c\\in \\mathbb{R}$, we have \n",
    "$$\n",
    "A\\left(c{\\bf x}_{h} + {\\bf x}_{p} \\right) = c A{\\bf x}_{h} + A{\\bf x}_{p} = {\\bf b}.\n",
    "$$\n",
    "* The only solution to $A^{T}{\\bf y}=0$ is ${\\bf y}=0$.  Then we say a matrix $A^{-1}$ exists, where $AA^{-1}=A^{-1}A=I$ and we have the **unique** solution \n",
    "$$\n",
    "{\\bf x} = A^{-1}{\\bf b}.\n",
    "$$\n",
    "\n",
    "There are several equivalent criteria which establish whether a square matrix $A$ has an inverse.  The most important are \n",
    "* The only solution to $A{\\bf x}=0$ is ${\\bf x}=0$.\n",
    "* The determinant of $A$ is non-zero, or $\\mbox{det}(A)\\neq 0$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Diagonal Matrices and Linear Algebra\n",
    "\n",
    "Suppose I want to solve, the albeit, simple differential equation \n",
    "\n",
    "$$\n",
    "T''(x) = f(x), ~ a < x < b, \n",
    "$$\n",
    "\n",
    "with the _boundary conditions_ $T(a)=u_{l}$ and $T(b)=u_{r}$.  We can think of this problem as modelling the temperature of a long, narrow corridor with walls at $a$ and $b$ kept at a fixed temperature and $f(x)$ represents a heat source in the hall.  To solve this problem, we introduce a discretized mesh with $N+1$ points where \n",
    "\n",
    "$$\n",
    "x_{j} = a + j\\delta x, ~ \\delta x = \\frac{b-a}{N}, ~ j=0,\\cdots,N.\n",
    "$$\n",
    "\n",
    "For the points $\\left\\{x_{j}\\right\\}_{j=1}^{N-1}$, we have the centered-difference approximations to the second derivative so that \n",
    "\n",
    "$$\n",
    "T''(x_{j}) \\approx \\frac{1}{(\\delta x)^{2}}\\left(T_{j-1} - 2T_{j} + T_{j+1}\\right), ~ j=1,\\cdots,N-1.\n",
    "$$\n",
    "\n",
    "with the boundary conditions\n",
    "\n",
    "$$\n",
    "T_{0} = u_{l}, ~ T_{N} = u_{r}.\n",
    "$$\n",
    "\n",
    "If I then give you the data $\\left\\{f_{j}\\right\\}_{j=1}^{N-1}$ where $f_{j}=f(x_{j})$, then we get the following linear algebra problem \n",
    "\n",
    "$$\n",
    "\\frac{1}{(\\delta x)^{2}}\\begin{pmatrix} -2 & 1 & &\\\\\n",
    "1 & -2 & 1 &\\\\\n",
    " & & \\ddots & & \\\\\n",
    " & & 1 & -2 & 1\\\\\n",
    " & & & 1 & -2\n",
    "\\end{pmatrix} \\begin{pmatrix} T_{1} \\\\ T_{2} \\\\ \\vdots \\\\ T_{N-2} \\\\ T_{N-1}\\end{pmatrix} = \\begin{pmatrix} f_{1} - \\frac{u_{l}}{(\\delta x)^{2}} \\\\ f_{2} \\\\ \\vdots \\\\ f_{N-2} \\\\ f_{N-1} - \\frac{u_{r}}{(\\delta x)^{2}} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using the code fragment below, write a program that finds $T$ given $a$, $b$, $N$, $f$, $u_{l}$, and $u_{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_find(a,b,Nvls,fvals,ul,ur):\n",
    "    Nint = int(Nvls)\n",
    "    dx = (b-a)/Nvls\n",
    "    idx2 = 1./(dx*dx)\n",
    "   \n",
    "    # Build the right-hand side of your problem\n",
    "    rhs = fvals[1:Nint]\n",
    "    rhs[0] -= ul*idx2 # include left-side boundary condition\n",
    "    rhs[Nint-2] -= ur*idx2 # include right-side boundary condition\n",
    "   \n",
    "    diag = -2.*idx2*np.ones(Nint-1)\n",
    "    odiag = idx2*np.ones(Nint-1)\n",
    "    data = np.array([diag,odiag,odiag])\n",
    "    dvals = np.array([0,-1,1])\n",
    "    Amat = spdiags(data, dvals, Nint-1, Nint-1)\n",
    "    Tvec = spsolve(Amat,rhs)\n",
    "   \n",
    "    return Tvec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffun = lambda x: np.cos(x)\n",
    "a = 0.\n",
    "b = 10.\n",
    "ul = 5.\n",
    "ur = 7.\n",
    "Nvls = 1000\n",
    "xvals = np.linspace(a,b,Nvls+1)\n",
    "fvals = ffun(xvals)\n",
    "\n",
    "tprofile = temp_find(a,b,Nvls,fvals,ul,ur)\n",
    "plt.plot(xvals[1:Nvls],tprofile)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$T(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "We also need a notion of the size of a vector.  We thus define the _2-norm_ of a vector ${\\bf x}\\in \\mathbb{R}^{n}$ to be $\\left|\\left|{\\bf x}\\right|\\right|_{2}$ where\n",
    "\n",
    "$$\n",
    "\\left|\\left|{\\bf x}\\right|\\right|_{2} = \\left(\\sum_{j=1}^{n}\\left|x_{j} \\right|^{2} \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|_{2} = 0 $ if and only if ${\\bf x} = {\\bf 0}$.\n",
    "* For any scalar values $\\lambda \\in \\mathbb{R}$, $\\left|\\left|\\lambda{\\bf x}\\right|\\right|_{2} = \\left|\\lambda\\right| \\left|\\left|{\\bf x}\\right|\\right|_{2}$.\n",
    "* If we define $\\hat{{\\bf x}} = {\\bf x}/\\left|\\left|{\\bf x}\\right|\\right|_{2}$, then $\\left|\\left|\\hat{{\\bf x}}\\right|\\right|_{2} = 1 $.\n",
    "\n",
    "We have some definitions to keep in mind about different types of matrices.  \n",
    "\n",
    "* If $m=n$, we say the matrix $A$ is square.  \n",
    "* If $m<n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _underdetermined_, i.e. there are fewer equations than there are unknowns.  \n",
    "* If $m>n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _overdetermined_, i.e. there are more equations than there are unknowns.  \n",
    "* For $m\\times n$ matrix $A$, we define its _transpose, $A^{T}$ to be the $n \\times m$ matrix with entries $A^{T}_{ij} = A_{ji}$.  Visually, we see that $A^{T}$ is flipped relative to $A$, i.e. \n",
    "$$\n",
    "A^{T} = \\begin{pmatrix} A_{11} & A_{21} & \\cdots & A_{m1} \\\\ A_{12} & A_{22} & \\cdots & A_{m2}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{1n} & A_{2n} & \\cdots & A_{mn} \\end{pmatrix},~ A^{T}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{n}\n",
    "$$\n",
    "* Using the rules of matrix multiplication, if $A$ is an $m\\times n$ matrix, then $A^{T}A$ is an $n \\times n$ matrix and $AA^{T}$ is a $m \\times m$ matrix.  Thus for an overdetermined problem, we can transform it into a square problem of the form \n",
    "$$\n",
    "A^{T}A {\\bf x} = A^{T}{\\bf b}.\n",
    "$$\n",
    "\n",
    "Underdetermined problems are a bit messier to cope with.  To do so, we need to get more comfortable with norms.  To that end then:\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|^{2}_{2} = {\\bf x}^{T}{\\bf x}$\n",
    "* Show $(A^{T}A)^{T} = A^{T}A$ and $(AA^{T})^{T} = AA^{T}$.  Matrices which are equal to their own transpose are called _symmetric_ or _self-adjoint_.\n",
    "* Suppose $A^{T}{\\bf y} = 0$.  Show that if a solution to $A{\\bf x}={\\bf b}$ exists, then $\\left<{\\bf b},{\\bf y}\\right>=0$.  \n",
    "* For ${\\bf x},{\\bf y} \\in \\mathbb{R}^{n}$, show the _Cauchy-Schwarz_ inequality \n",
    "$$\n",
    "\\left|\\left<{\\bf x},{\\bf y}\\right>\\right| = \\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\left(\\sum_{j=1}^{n}|x_{j}|^{2}\\right)^{1/2}\\left(\\sum_{j=1}^{n}|y_{j}|^{2}\\right)^{1/2} = \\left|\\left|{\\bf x}\\right|\\right|_{2}\\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "Note, the triangle inequality for real numbers, $|a+b|\\leq |a|+|b|$ gives us via an inductive argument that \n",
    "$$\n",
    "\\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\sum_{j=1}^{n}\\left|x_{j}\\right|\\left|y_{j}\\right|\n",
    "$$\n",
    "* Using the Cauchy-Schwarz inequality and the triangle inequality, where $|a+b|\\leq |a|+|b|$, show \n",
    "$$\n",
    "\\left|\\left|{\\bf x}+{\\bf y}\\right|\\right|_{2} \\leq \\left|\\left|{\\bf x}\\right|\\right|_{2} + \\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "\n",
    "### Eigenvalues and Symmetric Matrices\n",
    "\n",
    "We say that a constant $\\lambda \\in \\mathbb{C}$ is an _eigenvalue_ of a square matrix $A$ if there exists some vector ${\\bf v}$ such that \n",
    "$$\n",
    "A{\\bf v} = \\lambda {\\bf v}.\n",
    "$$\n",
    "We call ${\\bf v}$ an _eigenvector_.  Note, the above equation is equivalent to \n",
    "$$\n",
    "(A - \\lambda I){\\bf v} = 0,\n",
    "$$\n",
    "which is equivalent to requiring that $\\mbox{det}(A-\\lambda I)=0$.  Note, we are usually obliged to worry about complex eigenvalues and eigenvectors for even real matrices.  \n",
    "\n",
    "_Problem_: For \n",
    "$$\n",
    "A = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0\\end{pmatrix},\n",
    "$$\n",
    "find its eigenvalues and eigenvectors.  \n",
    "\n",
    "However, we are only going to work with symmetric matrices where $A^{T}=A$.  In this case, one can show that $\\lambda$ and its corresponding ${\\bf v}$ are real.  This essentially comes from the formula\n",
    "$$\n",
    "\\lambda \\left|\\left|{\\bf v}\\right|\\right|^{2} = \\left<A{\\bf v},{\\bf v} \\right> = \\left<{\\bf v},A{\\bf v} \\right>.\n",
    "$$\n",
    "Thus, we get for symmetric matrices the formula for the eigenvalues $\\lambda$\n",
    "$$\n",
    "\\lambda = \\left<A\\hat{{\\bf v}},\\hat{{\\bf v}} \\right>, ~ \\hat{{\\bf v}} = {\\bf v}/ \\left|\\left|{\\bf v}\\right|\\right|.\n",
    "$$\n",
    "\n",
    "_Problem_: Show \n",
    "* For symmetric matrix $A$ that if it has two unequal eigenvalues, say $\\lambda_{1}$ and $\\lambda_{2}$, with \n",
    "$$\n",
    "A{\\bf v}_{1} = \\lambda_{1}{\\bf v}_{1}, ~ A{\\bf v}_{2} = \\lambda_{2}{\\bf v}_{2}\n",
    "$$\n",
    "that $\\left<{\\bf v}_{1},{\\bf v}_{2} \\right>=0$.\n",
    "* Suppose that for a symmetric matrix $A$, we find two eigenvectors, say ${\\bf v}_{1}$ and ${\\bf v}_{2}$ for the same eigenvalue $\\lambda$.  Show that one can find two eigenvectors $\\hat{\\bf v}_{1}$ and $\\hat{\\bf v}_{2}$ such that \n",
    "$$\n",
    "A\\hat{\\bf v}_{j} = \\lambda \\hat{\\bf v}_{j}, ~ \\left<\\hat{{\\bf v}}_{1}, \\hat{{\\bf v}}_{2}\\right>=0.\n",
    "$$\n",
    "* Suppose one has found $j-1$ eigenvalues and eigenvectors, say $\\lambda_{l}$ and $\\hat{{\\bf v}}_{l}$, $l=1,\\cdots,j-1$, of the matrix $A$, and $\\left<\\hat{{\\bf v}}_{l},\\hat{{\\bf v}}_{k} \\right>=\\delta_{lk}$.  Note, we do not require that the values $\\lambda_{l}$ all be distinct, i.e. eigenvalues can repeat.  Suppose one has a unit vector $\\hat{{\\bf x}}$ such that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\hat{{\\bf v}}_{l} \\right> = 0.\n",
    "$$\n",
    "Show that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\sum_{l=1}^{j-1}c_{l}\\hat{{\\bf v}}_{l} \\right>  = 0, ~ c_{l} \\in \\mathbb{R}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Splines\n",
    "\n",
    "So, while linear splines clearly work, they do not do so in a terribly efficient way.  To improve on this then, as before, we start our discussion with a data set \n",
    "\n",
    "$$\n",
    "\\left\\{x_{j},f_{j}\\right\\}_{j=0}^{n}.\n",
    "$$\n",
    "\n",
    "We now define our _cubic splines_ $S_{j}(x)$ to be third order polynomials, i.e. \n",
    "\n",
    "$$\n",
    "S_{j}(x) = a_{j}(x-x_{j})^{3} + b_{j}(x-x_{j})^{2} + c_{j}(x-x_{j}) + d_{j},\n",
    "$$\n",
    "\n",
    "such that \n",
    "\n",
    "\\begin{align}\n",
    "S_{j}(x_{j}) = & f_{j}, ~ j=0,\\cdots,n-1\\\\\n",
    "S_{j}(x_{j+1}) = & S_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \\\\\n",
    "S'_{j}(x_{j+1}) = & S'_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \\\\\n",
    "S''_{j}(x_{j+1}) = & S''_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2 \n",
    "\\end{align}\n",
    "\n",
    "which is to say, we require that we interpolate the data, and the each spline as as its first and second derivatives be continuous at each node.  Finally, we require that \n",
    "\n",
    "$$\n",
    "S_{n-1}(x_{n}) = f_{n}, ~ S''_{0}(x_{0})=0, ~ S''_{n-1}(x_{n}) = 0.\n",
    "$$\n",
    "\n",
    "We readily see then that $d_{j}= f_{j}$.  Define \n",
    "\n",
    "$$\n",
    "\\delta x_{j} = x_{j+1} -x_{j}, ~ \\delta f_{j} = f_{j+1} - f_{j}.\n",
    "$$\n",
    "\n",
    "Then from above we get the system of equations for  $j=0,\\cdots,n-2$\n",
    "\n",
    "\\begin{align}\n",
    "a_{j}(\\delta x_{j})^{2} + b_{j}\\delta x_{j} + c_{j} = & \\frac{\\delta f_{j}}{\\delta x_{j}},\\\\\n",
    "3a_{j}(\\delta x_{j})^{2} + 2b_{j}\\delta x_{j} + c_{j} = & c_{j+1},\\\\\n",
    "3a_{j}\\delta x_{j} + b_{j} = & b_{j+1} \n",
    "\\end{align}\n",
    "\n",
    "The end point conditions give us\n",
    "\n",
    "$$\n",
    "a_{n-1}\\left(\\delta x_{n-1}\\right)^{2} + b_{n-1}\\delta x_{n-1} + c_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "b_{0}=0, ~ 3a_{n-1}\\delta x_{n-1} + b_{n-1} = 0.\n",
    "$$\n",
    "\n",
    "Solving for $a_{j}$ gives us, \n",
    "\n",
    "$$\n",
    "a_{j} = \\frac{1}{\\delta x_{j}^{2}}\\left(\\frac{\\delta f_{j}}{\\delta x_{j}} - c_{j} - b_{j}\\delta x_{j}\\right), ~ j=0,\\cdots,n-1,\n",
    "$$\n",
    "\n",
    "and in turn we then find that \n",
    "\n",
    "\\begin{align}\n",
    "3\\frac{\\delta f_{j}}{\\delta x_{j}} - b_{j}\\delta x_{j} - 2c_{j} = & c_{j+1}, ~ j=0,\\cdots,n-2\\\\\n",
    "3\\frac{\\delta f_{j}}{(\\delta x_{j})^{2}} - 3\\frac{c_{j}}{\\delta x_{j}} - 2b_{j} = & b_{j+1}, ~j=0,\\cdots,n-2,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "c_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}} - \\frac{2}{3}\\delta x_{n-1}b_{n-1}.\n",
    "$$\n",
    "\n",
    "Likewise, solving for $c_{j}$ then gives us\n",
    "\n",
    "$$\n",
    "c_{j} = \\frac{\\delta f_{j}}{\\delta x_{j}} - \\frac{\\delta x_{j}}{3}\\left( 2b_{j} + b_{j+1}\\right), ~ j=0,\\cdots,n-2.\n",
    "$$\n",
    "\n",
    "Ultimately then, we arrive at the system of equations, for $j=1,\\cdots,n-3$, \n",
    "\n",
    "$$\n",
    "\\frac{\\delta x_{j}}{3} b_{j} + \\frac{2}{3}\\left(\\delta x_{j} + \\delta x_{j+1} \\right)b_{j+1} + \\frac{\\delta x_{j+1}}{3} b_{j+2} = \\frac{\\delta f_{j+1}}{\\delta x_{j+1}} - \\frac{\\delta f_{j}}{\\delta x_{j}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{2}{3}\\left(\\delta x_{0} + \\delta x_{1} \\right)b_{1} + \\frac{\\delta x_{1}}{3} b_{2} = \\frac{\\delta f_{1}}{\\delta x_{1}} - \\frac{\\delta f_{0}}{\\delta x_{0}},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\delta x_{n-2}}{3} b_{n-2} + \\frac{2}{3}\\left(\\delta x_{n-2} + \\delta x_{n-1} \\right)b_{n-1} = \\frac{\\delta f_{n-1}}{\\delta x_{n-1}} - \\frac{\\delta f_{n-2}}{\\delta x_{n-2}}.\n",
    "$$\n",
    "\n",
    "At this point, we should talk about solving the problem $A{\\bf b}=\\tilde{{\\bf f}}$ where $A$ is a self-adjoint tridiagonal matrix.  In other words, defining the vectors ${\\bf b}$ and $\\tilde{{\\bf f}}$ where\n",
    "\n",
    "$$\n",
    "{\\bf b} = \\begin{pmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n-1} \\end{pmatrix}, ~ \\tilde{{\\bf f}} = \\begin{pmatrix} \\frac{\\delta f_{1}}{\\delta x_{1}} - \\frac{\\delta f_{0}}{\\delta x_{0}} \\\\ \\frac{\\delta f_{2}}{\\delta x_{2}} - \\frac{\\delta f_{1}}{\\delta x_{1}} \\\\ \\vdots \\\\ \\frac{\\delta f_{n-1}}{\\delta x_{n-1}} - \\frac{\\delta f_{n-2}}{\\delta x_{n-2}}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "we see that $A$ is an $(n-1)\\times (n-1)$ symmetric ($A^{T}=A$) matrix where\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} \\frac{2}{3}(\\delta x_{0} + \\delta x_{1}) & \\frac{1}{3}\\delta x_{1} & &\\\\\n",
    "\\frac{1}{3}\\delta x_{1} & \\frac{2}{3}(\\delta x_{1} + \\delta x_{2}) & \\frac{1}{3}\\delta x_{2} &\\\\\n",
    " & & \\ddots & & \\\\\n",
    " & & \\frac{1}{3}\\delta x_{n-3} & \\frac{2}{3}(\\delta x_{n-3} + \\delta x_{n-2}) & \\frac{1}{3}\\delta x_{n-2}\\\\\n",
    " & & & \\frac{1}{3}\\delta x_{n-2} & \\frac{2}{3}(\\delta x_{n-2} + \\delta x_{n-1})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In code, we proceed as below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_maker(xvals,fvals,qvals):\n",
    "    # m = fvals.size\n",
    "    # note, from above, n = m-1\n",
    "    \n",
    "    n = fvals.size - 1\n",
    "    df = fvals[1:]-fvals[0:n]\n",
    "    dx = xvals[1:]-xvals[0:n]\n",
    "    dfdx = df/dx\n",
    "    svals = np.zeros(ivals.size)\n",
    "    \n",
    "    rhs = dfdx[1:] - dfdx[0:n-1]\n",
    "    diag = 2./3.*(dx[1:] + dx[0:n-1])\n",
    "    data = np.array([diag,dx[1:]/3.,dx[0:n-1]/3.])\n",
    "    dvals = np.array([0,-1,1])\n",
    "    Amat = spdiags(data, dvals, n-1, n-1)\n",
    "    bvec = spsolve(Amat,rhs)\n",
    "    \n",
    "    bvec = np.append(0.,bvec)\n",
    "    \n",
    "    cvec = dfdx - 2./3.*dx*bvec - dx/3.*np.append(bvec[1:],0.)\n",
    "    avec = (dfdx - dx*bvec - cvec)/(dx**2.)\n",
    "    \n",
    "    for jj in range(1,n+1):\n",
    "        \n",
    "        indsr = qvals < xvals[jj] \n",
    "        indsl = qvals >= xvals[jj-1]\n",
    "        inds = indsl*indsr\n",
    "        \n",
    "        dxloc = qvals[inds] - xvals[jj-1]\n",
    "        svals[inds] = avec[jj-1]*dxloc**3. + bvec[jj-1]*dxloc**2. + cvec[jj-1]*dxloc + fvals[jj-1]\n",
    "        \n",
    "    return svals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the example from above in which \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+x^{2}}, ~ -1\\leq x \\leq 1,\n",
    "$$\n",
    "\n",
    "we can now test our spline approximation scheme.  As we show, it can be very accurate, and while our arbitrary choice of enforcing zero curvature at the endpoints does cost us some amount of accuracy, we do not have anything resembling the problems we saw above with Lagrange interpolation.  Thus, splines offer us an accurate, efficient, and flexible means of interpolating data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvs = np.linspace(-1.,1.,int(1e3)+1)\n",
    "fvs = 1./(1.+xvs**2.)\n",
    "ivals = np.linspace(-.99,.99,int(5e3))\n",
    "ftrue = 1./(1.+ivals**2.)\n",
    "\n",
    "svals = spline_maker(xvs,fvs,ivals)\n",
    "#plt.plot(ivals,svals,ls='-',color='k')\n",
    "#plt.plot(ivals,ftrue,ls='--',color='r')\n",
    "plt.plot(ivals,np.ma.log10(np.abs(ftrue-svals)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
