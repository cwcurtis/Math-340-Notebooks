{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Quadrature\n",
    "\n",
    "Okay, so now we are going to get into one of the biggest reasons we do math on computers, and that is to numerically compute integrals.  What we are attempting to do then is, for a given function $f(x)$, find approximations to \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\int_{a}^{b}f(x) dx.\n",
    "$$\n",
    "\n",
    "To do this, we first need to introduce the notion of a mesh $\\left\\{x_{i}\\right\\}_{j=0}^{N}$ where\n",
    "\n",
    "$$\n",
    "x_{0} =a, ~ x_{N} = b, ~ x_{i+1}-x_{i} = \\delta x = \\frac{b-a}{N}.\n",
    "$$\n",
    "\n",
    "## The Trapezoid Method\n",
    "\n",
    "Thus, our first attempt at developing an approximation scheme will be to find approximations over intervals $[x_{i},x_{i+1}]$.  Then we will use the identity\n",
    "\n",
    "$$\n",
    "\\int_{a}^{b} f(x) dx = \\sum_{i=0}^{N-1}\\int_{x_{i}}^{x_{i+1}}f(x)dx.\n",
    "$$\n",
    "\n",
    "Now, on each interval $[x_{i},x_{i+1}]$, we now replace $f(x)$ with a straight-line approximation which connects the points $(x_{i},f(x_{i}))$ and $(x_{i+1},f(x_{i+1}))$.\n",
    "\n",
    "![trap](https://upload.wikimedia.org/wikipedia/commons/d/d1/Integration_num_trapezes_notation.svg)\n",
    "\n",
    "What we are saying here is that we make the assumption that\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(x_{i}) + \\frac{(f(x_{i+1})-f(x_{i}))}{\\delta x}(x-x_{i}), ~ x\\in[x_{i},x_{i+1}].\n",
    "$$\n",
    "\n",
    "The advantage of doing this is that we can readily compute the integral of the right-hand side of our approximation.\n",
    "\n",
    "\n",
    "_Problem_: Using the approximation to the function, find an approximation to the integral of $f(x)$ over the interval $[x_{i},x_{i+1}]$.  \n",
    "\n",
    "So as we discussed in class, this is given by \n",
    "\n",
    "$$\n",
    "\\int_{x_{j}}^{x_{j+1}} f(x) dx \\approx \\frac{\\delta x}{2}\\left(f(x_{j}) + f(x_{j+1})\\right)\n",
    "$$\n",
    "\n",
    "_Problem_: Combine your approximations over each sub interval $[x_{i},x_{i+1}]$ to build an approximation for the integral of $f(x)$ on $[a,b]$.    \n",
    "\n",
    "Again, as discussed in class, this becomes \n",
    "\n",
    "$$\n",
    "\\int_{a}^{b}f(x) dx \\approx \\frac{\\delta x}{2}\\left(f(a) + f(b) + 2\\sum_{j=1}^{N-1}f(x_{j})\\right)\n",
    "$$\n",
    "\n",
    "_Problem_: Code it.  Test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapezoid_method(a,b,N,f):\n",
    "    Nint = int(N)\n",
    "    xvals = np.linspace(a,b,Nint+1)    \n",
    "    fvals = f(xvals) \n",
    "    dx = (b-a)/N\n",
    "    return dx/2. * ( fvals[0] + fvals[-1] + 2.*np.sum(fvals[1:-1]) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(xvals):\n",
    "    fvals = (np.cos(xvals))**2.\n",
    "    return fvals\n",
    "# test your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589794\n"
     ]
    }
   ],
   "source": [
    "print(trapezoid_method(0.,2.*np.pi,100,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis for the Trapezoid Method\n",
    "\n",
    "If we denote the approximation to $T_{[a,b]}(f)$ via the Trapezoid Method as $A_{N}(f)$, we want to figure out how the difference between these two things behaves as a function of $N$, or equivalently, $\\delta x$.  We suppose that \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{N}(f) + C\\left(\\delta x\\right)^{p}, ~ \\delta x = \\frac{b-a}{N}.\n",
    "$$\n",
    "\n",
    "So how can we find $p$?  Well, our friend the logarithm returns since we see that \n",
    "\n",
    "$$\n",
    "\\log_{10}\\left|T_{[a,b]}(f) - A_{N}(f) \\right| = \\log_{10}C + p\\left(\\log_{10}(b-a) - \\log_{10} N \\right)\n",
    "$$\n",
    "\n",
    "Thus, if we compare the log of the error to the log of the number of intervals in our mesh, then the slope of that line should be the rate of decay, or how the error decreases as we increase the number of points in the mesh. \n",
    "\n",
    "_Problem_: Write code to find $p$.  This means you need to think of a test case for which you know the answer and then generate a series of comparisons which allow you to infer a trend.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_test_trapezoid():\n",
    "    f = lambda x: (np.cos(x))**2.\n",
    "    a,b = 0., np.pi/4.\n",
    "    tval = .5*(np.pi/4. + .5)\n",
    "    Nvals = np.array([1e1, 1e2, 1e3, 1e4, 1e5])\n",
    "    Evals = np.zeros(Nvals.size)\n",
    "    for jj in range(Nvals.size):\n",
    "        Evals[jj] = tval - trapezoid_method(a,b,Nvals[jj],f) \n",
    "    xvals = np.log10(Nvals)\n",
    "    plt.plot(xvals,np.log10(Evals))\n",
    "    novals = Evals.size\n",
    "    slopes = np.log10(np.abs(Evals[1:])) - np.log10(np.abs(Evals[0:-1])) \n",
    "    print(np.min(slopes))\n",
    "    print(np.max(slopes))\n",
    "    print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0001768787071543\n",
      "-1.9999885567308464\n",
      "-2.0000418002299023\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_test_trapezoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Simpon's Method\n",
    "\n",
    "So, if lines were great, what if we used a more accurate means of approximating a function?  To do this, we will work over three mesh points, say $[x_{j},x_{j+2}]$, and then make the approximation\n",
    "\n",
    "$$\n",
    "f(x) \\approx a_{j+1}(x-x_{j+1})^{2} + b_{j+1}(x-x_{j+1}) + c_{j+1},\n",
    "$$\n",
    "\n",
    "with the interpolation requirements\n",
    "\n",
    "\\begin{align}\n",
    "y_{j+1}(x_{j}) = & f(x_{j})\\\\\n",
    "y_{j+1}(x_{j+1}) = & f(x_{j+1})\\\\\n",
    "y_{j+1}(x_{j+2}) = & f(x_{j+2})\n",
    "\\end{align}\n",
    "\n",
    "![simp](https://jeremykun.files.wordpress.com/2011/12/simpson.png?w=1800)\n",
    "\n",
    "Then, using this approximation, we approximate the integral of $f$ over $[x_{j},x_{j+2}]$ via \n",
    "\n",
    "$$\n",
    "\\int_{x_{j}}^{x_{j+2}}f(x) dx \\approx \\int_{x_{j}}^{x_{j+2}} y_{j+1}(x)dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpson_method(a,b,N,f):\n",
    "    Nint = int(N)\n",
    "    TNint = 2*N\n",
    "    xvals = np.linspace(a,b,TNint+1)\n",
    "    fvals = f(xvals)\n",
    "    dx = (b-a)/(2.*N)\n",
    "    return dx/3.*(fvals[0] + fvals[-1] + 2.*np.sum(fvals[2:-2:2]) + 4.*np.sum(fvals[1:-1:2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793\n"
     ]
    }
   ],
   "source": [
    "print( simpson_method(0., 2.*np.pi, 20, lambda x: (np.cos(x))**2. ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adaptive Quadrature\n",
    "\n",
    "So, as we you show in the homework, letting\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\int_{a}^{b}f(x)dx\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "A_{[a,b]}(f) = \\frac{(b-a)}{6}\\left(f(a) + 4f(c) + f(b)\\right), ~ c = \\frac{a+b}{2},\n",
    "$$\n",
    "\n",
    "Simpson's method gives us\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) \\approx A_{[a,b]}(f) + C_{0}(b-a)^{5}.\n",
    "$$\n",
    "\n",
    "Note, the true story is actually a bit more complicated, and we really should write\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{[a,b]}(f) + C_{0}(b-a)^{5} + C_{1}(b-a)^{6} + \\cdots.\n",
    "$$\n",
    "\n",
    "In this vein, if we split $T_{[a,b]}(f)$ into \n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = T_{[a,c]}(f) + T_{[c,b]}(f), ~ c = \\frac{a+b}{2},\n",
    "$$\n",
    "\n",
    "then we we can build a better approximation by using\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = A_{[a,c]}(f) + A_{[c,b]}(f) + \\frac{C_{0}}{16}(b-a)^{5} + \\cdots\n",
    "$$\n",
    "\n",
    "Now, where this gets really tricky is that we can do better yet still.  Here is how.  From my new approximation, I can write\n",
    "\n",
    "$$\n",
    "16 T_{[a,b]}(f) = 16\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) + C_{0}(b-a)^{5} + \\cdots\n",
    "$$\n",
    "\n",
    "Using the old approximation, I get \n",
    "\n",
    "$$\n",
    "15 T_{[a,b]}(f) = 16\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - A_{[a,b]}(f) + \\cdots\n",
    "$$\n",
    "\n",
    "and thus we get the new and improved approximation\n",
    "\n",
    "$$\n",
    "T_{[a,b]}(f) = \\frac{16}{15}\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - \\frac{1}{15}A_{[a,b]}(f) + \\cdots\n",
    "$$\n",
    "\n",
    "So now think about that.  By building two different approximations, I am able to get a third, yet better one.  But we can also ask another kind of question.  What if \n",
    "\n",
    "$$\n",
    "\\left|\\left(A_{[a,c]}(f) + A_{[c,b]}(f)\\right) - A_{[a,b]}(f)\\right| < \\mbox{tol} ~ \\mbox{?}\n",
    "$$\n",
    "\n",
    "As in, okay, I make two approximations, one okay, one better.  But the difference between them is not so large?  Then what is the point of continuing to make smaller subdivisions of the interval when I am actually happy with what I have got?  So, how would I implement a method based on this line of thinking?  And what would it get me?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief aside on recursion in Python\n",
    "\n",
    "While we haven't had too much use for it yet, it is worth while to see some relatively simple examples of more classic recursive programs in Python. What I mean by this is the following.  Say we wanted to compute the factorial of an integer $n$, i.e. we want to find $n!$.  Well, then we use the recursive formula\n",
    "\n",
    "$$\n",
    "n! = n(n-1)!\n",
    "$$\n",
    "\n",
    "along with the stopping criteria that $0!=1$.  In code, this looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_factorial(n):\n",
    "    if n==0: # Here we implement the stopping criteria which ends the recursion.  \n",
    "        return 1\n",
    "    else: # Here, having not reached a stoppping point, we recurse\n",
    "        return #add code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"An example for n=3, 3!=%d\"%r_factorial(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning then to our adaptive, recursive integration scheme, we use the skeleton below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_quad_comp(f,a,b,c,fa,fb,fc,Aab,tol,ptlst):\n",
    "    \n",
    "    d = #add code here \n",
    "    e = #add code here \n",
    "    ptlst.append(d)\n",
    "    ptlst.append(e)\n",
    "    fd = f(d)\n",
    "    fe = f(e)\n",
    "    dx = d-a\n",
    "    Aac = #add code here \n",
    "    Acb = #add code here \n",
    "    if np.abs(#add code here )<=tol: # This is our stopping condition in terms of tol\n",
    "        return (16./15.*(Aac+Acb)-Aab/15.)\n",
    "    else: # This is if we do not meet the tol condition \n",
    "        F1 = #add code here \n",
    "        F2 = #add code here \n",
    "        return F1 + F2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_quad(f,a,b,tol):\n",
    "    c = (a+b)/2.\n",
    "    fa = #add code here \n",
    "    fb = #add code here \n",
    "    fc = #add code here \n",
    "    dx = c-a\n",
    "    Aab = #add code here \n",
    "    ptlst = [a, c, b]\n",
    "    ival = #add code here \n",
    "    '''\n",
    "    # Plotting commands which are not essential to understand at this time.  \n",
    "    ptlst.sort()\n",
    "    for jj in xrange(0,len(ptlst)):\n",
    "        fval = f(ptlst[jj])\n",
    "        if fval > 0:\n",
    "            plt.plot([ptlst[jj],ptlst[jj]],[0,fval],color='r',ls='--')\n",
    "        else:\n",
    "            plt.plot([ptlst[jj],ptlst[jj]],[fval,0],color='r',ls='--')\n",
    "    xvals = np.linspace(a,b,int(1e3))\n",
    "    yvals = f(xvals)\n",
    "    zvals = np.zeros(xvals.size)\n",
    "    plt.plot(xvals,yvals,color='k',ls='-')\n",
    "    plt.plot(xvals,zvals,color='k',ls='-')\n",
    "    '''\n",
    "    return ival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example with rapid oscillations\n",
    "faq = lambda x : np.sin(x**2.)\n",
    "print(adap_quad(faq,0.,3.6,1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_error_test(a,b,f):\n",
    "    tval = (b-a)/2. + (np.sin(2.*b)-np.sin(2.*a))/4.\n",
    "    tolvals = np.array([1e-1,1e-2,1e-3,1e-4])\n",
    "    Evals = np.zeros(tolvals.size)\n",
    "    for jj in xrange(0,tolvals.size):\n",
    "        Evals[jj] = np.log10(np.abs(tval - adap_quad(f,a,b,tolvals[jj])))\n",
    "    xvals = -np.log10(tolvals)\n",
    "    plt.plot(xvals,Evals)\n",
    "    plt.xlabel('$-log_{10}(tol)$')\n",
    "    plt.ylabel('$log_{10}(Error)$')\n",
    "    \n",
    "    novals = Evals.size\n",
    "    slopes = (Evals[1:novals]-Evals[0:novals-1])/(xvals[1:novals]-xvals[0:novals-1])\n",
    "    print(np.min(slopes))\n",
    "    print(np.max(slopes))\n",
    "    print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adap_error_test(1.,2.,csq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
