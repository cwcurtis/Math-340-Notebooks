{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrices and Finite Differences\n",
    "\n",
    "So, starting from our usual $N+1$ point, equi-spaced mesh, $x_{j}=a+j\\delta x$, $\\delta x = (b-a)/N$, we have our second-order, finite-difference approximation to $f'(x_{j})$ given by \n",
    "\n",
    "$$\n",
    "f'(x_{j}) = \\frac{1}{2\\delta x}\\left(f_{j+1}-f_{j-1}\\right), ~ 1\\leq j \\leq N-1.  \n",
    "$$\n",
    "\n",
    "Now, how can we think about this in a global sense?  The best way is through linear algebra, where we write\n",
    "\n",
    "$$\n",
    "\\frac{1}{2\\delta x}\\begin{pmatrix}-1 & 0 & 1 & 0 & \\cdots & 0\\\\ \n",
    "0 & -1 & 0 & 1 & \\cdots & 0\\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & \\vdots\\\\\n",
    "0 & \\cdots & 0 & -1 & 0 & 1\\\\\n",
    "\\end{pmatrix}\\begin{pmatrix}f(x_{0})\\\\ f(x_{1}) \\\\ f(x_{2}) \\\\ \\vdots \\\\ f(x_{N})\\end{pmatrix} \\approx \\begin{pmatrix}f'(x_{1})\\\\ f'(x_{2}) \\\\  f'(x_{3}) \\\\\\vdots \\\\ f'(x_{N-1})\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "As we can see, the matrix in the left hand side is mostly just zeros.  Such a matrix is described as _sparse_.  How we define and work with sparse matrices is special within NumPy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_first_derivative(fvals, dx):\n",
    "    Np1 = fvals.size\n",
    "    ovec = np.ones(Np1)\n",
    "    zvec = np.zeros(Np1)\n",
    "    dmat = 1./(2.*dx)*spdiags(np.array([-ovec,zvec,ovec]),np.array([0,1,2]),Np1-2,Np1)\n",
    "    return dmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts = 5\n",
    "xvals = np.linspace(0.,2.*np.pi,Npts+1)\n",
    "dx = xvals[1] - xvals[0]\n",
    "fvals = np.sin(xvals) # use f(x) = sin(x) for your test function\n",
    "dftrue = np.cos(xvals)\n",
    "dmat = sparse_first_derivative(fvals, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dmat.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about NumPy arrays and vectors.\n",
    "\n",
    "So when we write `fvals=np.sin(xvals)`, that generates a NumPy array.  But an array is not something we can multiply by a matrix.  For that, we need a vector.  The easiest way to do this is through the somewhat strange command `fvals.reshape(-1,1)`. See the example below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fvals)\n",
    "print(fvals.shape)\n",
    "print(fvals.reshape(-1,1))\n",
    "print((fvals.reshape(-1,1)).shape)\n",
    "dfapprox = dmat @ fvals.reshape(-1,1)\n",
    "print(dfapprox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xvals[1:-1], dftrue[1:-1],color='k',label=\"True Derivative\")\n",
    "plt.plot(xvals[1:-1], dfapprox,color='r',label=\"Approx. Derivative\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving a Boundary-Value Problem\n",
    "\n",
    "Suppose I want to solve, the albeit, simple differential equation \n",
    "\n",
    "$$\n",
    "T''(x) = f(x), ~ a < x < b, \n",
    "$$\n",
    "\n",
    "with the _boundary conditions_ $T(a)=u_{l}$ and $T(b)=u_{r}$.  We can think of this problem as modelling the temperature of a long, narrow corridor with walls at $a$ and $b$ kept at a fixed temperature and $f(x)$ represents a heat source in the hall.  To solve this problem, we introduce a discretized mesh with $N+1$ points where \n",
    "\n",
    "$$\n",
    "x_{j} = a + j\\delta x, ~ \\delta x = \\frac{b-a}{N}, ~ j=0,\\cdots,N.\n",
    "$$\n",
    "\n",
    "For the points $\\left\\{x_{j}\\right\\}_{j=1}^{N-1}$, we have the centered-difference approximations to the second derivative so that \n",
    "\n",
    "$$\n",
    "T''(x_{j}) \\approx \\frac{1}{(\\delta x)^{2}}\\left(T_{j-1} - 2T_{j} + T_{j+1}\\right), ~ j=1,\\cdots,N-1.\n",
    "$$\n",
    "\n",
    "with the boundary conditions\n",
    "\n",
    "$$\n",
    "T_{0} = u_{l}, ~ T_{N} = u_{r}.\n",
    "$$\n",
    "\n",
    "If I then give you the data $\\left\\{f_{j}\\right\\}_{j=1}^{N-1}$ where $f_{j}=f(x_{j})$, then we get the following linear algebra problem \n",
    "\n",
    "$$\n",
    "\\frac{1}{(\\delta x)^{2}}\\begin{pmatrix} -2 & 1 & &\\\\\n",
    "1 & -2 & 1 &\\\\\n",
    " & & \\ddots & & \\\\\n",
    " & & 1 & -2 & 1\\\\\n",
    " & & & 1 & -2\n",
    "\\end{pmatrix} \\begin{pmatrix} T_{1} \\\\ T_{2} \\\\ \\vdots \\\\ T_{N-2} \\\\ T_{N-1}\\end{pmatrix} = \\begin{pmatrix} f_{1} - \\frac{u_{l}}{(\\delta x)^{2}} \\\\ f_{2} \\\\ \\vdots \\\\ f_{N-2} \\\\ f_{N-1} - \\frac{u_{r}}{(\\delta x)^{2}} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Using the code fragment below, write a program that finds $T$ given $a$, $b$, $N$, $f$, $u_{l}$, and $u_{r}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_find(a,b,Nvls,fvals,ul,ur):\n",
    "    Nint = int(Nvls)\n",
    "    dx = (b-a)/Nvls\n",
    "    idx2 = 1./(dx*dx)\n",
    "   \n",
    "    # Build the right-hand side of your problem\n",
    "    rhs = fvals[1:Nint]\n",
    "    rhs[0] -= ul*idx2 # include left-side boundary condition\n",
    "    rhs[Nint-2] -= ur*idx2 # include right-side boundary condition\n",
    "   \n",
    "    diag = -2.*idx2*np.ones(Nint-1)\n",
    "    odiag = idx2*np.ones(Nint-1)\n",
    "    data = np.array([diag,odiag,odiag])\n",
    "    dvals = np.array([0,-1,1])\n",
    "    Amat = spdiags(data, dvals, Nint-1, Nint-1)\n",
    "    Tvec = spsolve(Amat,rhs)\n",
    "   \n",
    "    return Tvec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffun = lambda x: np.cos(x)\n",
    "a = 0.\n",
    "b = 10.\n",
    "ul = 5.\n",
    "ur = 7.\n",
    "Nvls = 1000\n",
    "xvals = np.linspace(a,b,Nvls+1)\n",
    "fvals = ffun(xvals)\n",
    "\n",
    "tprofile = temp_find(a,b,Nvls,fvals,ul,ur)\n",
    "plt.plot(xvals[1:Nvls],tprofile)\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$T(x)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra Theory - Just in case\n",
    "\n",
    "### Solvability for Square Matrices\n",
    "\n",
    "* Note, for ${\\bf x},{\\bf y} \\in \\mathbb{R}^{n}$, we define the _inner-product_  $\\left<{\\bf x},{\\bf y}\\right>$ to be\n",
    "$$\n",
    "\\left<{\\bf x},{\\bf y}\\right> = {\\bf y}^{T}{\\bf x} = \\sum_{j=1}^{n}y_{j}x_{j}.\n",
    "$$\n",
    "Show $\\left<{\\bf x},{\\bf y}\\right> = \\left<{\\bf y},{\\bf x}\\right>$, and for a square matrix $A$ that $\\left<A{\\bf x},{\\bf y}\\right> = \\left<{\\bf x},A^{T}{\\bf y}\\right>$.     \n",
    "\n",
    "We now suppose that in our $A{\\bf x} = {\\bf b}$ problem that $A$ is a $n\\times n$ square matrix of real values.  In general we have three outcomes to the question of whether a solution ${\\bf x}$ exists to this problem\n",
    "* There exists ${\\bf y}$ such that $A^{T}{\\bf y}=0$ but $\\left<{\\bf b},{\\bf y} \\right>\\neq 0$.  Then **no** solution ${\\bf x}$ exists to the problem.  \n",
    "* $\\left<{\\bf b},{\\bf y} \\right> = 0$ for all ${\\bf y}\\neq 0$ and $A^{T}{\\bf y} = 0$.  Then an **infinite** number of solutions exists of the form \n",
    "$$\n",
    "{\\bf x} = {\\bf x}_{h} + {\\bf x}_{p}, \n",
    "$$ \n",
    "where $A{\\bf x}_{h} = 0$ and $A{\\bf x}_{p}={\\bf b}$.  Note, we have an infinite number of solutions since for any constant $c\\in \\mathbb{R}$, we have \n",
    "$$\n",
    "A\\left(c{\\bf x}_{h} + {\\bf x}_{p} \\right) = c A{\\bf x}_{h} + A{\\bf x}_{p} = {\\bf b}.\n",
    "$$\n",
    "* The only solution to $A^{T}{\\bf y}=0$ is ${\\bf y}=0$.  Then we say a matrix $A^{-1}$ exists, where $AA^{-1}=A^{-1}A=I$ and we have the **unique** solution \n",
    "$$\n",
    "{\\bf x} = A^{-1}{\\bf b}.\n",
    "$$\n",
    "\n",
    "There are several equivalent criteria which establish whether a square matrix $A$ has an inverse.  The most important are \n",
    "* The only solution to $A{\\bf x}=0$ is ${\\bf x}=0$.\n",
    "* The determinant of $A$ is non-zero, or $\\mbox{det}(A)\\neq 0$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "We also need a notion of the size of a vector.  We thus define the _2-norm_ of a vector ${\\bf x}\\in \\mathbb{R}^{n}$ to be $\\left|\\left|{\\bf x}\\right|\\right|_{2}$ where\n",
    "\n",
    "$$\n",
    "\\left|\\left|{\\bf x}\\right|\\right|_{2} = \\left(\\sum_{j=1}^{n}\\left|x_{j} \\right|^{2} \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|_{2} = 0 $ if and only if ${\\bf x} = {\\bf 0}$.\n",
    "* For any scalar values $\\lambda \\in \\mathbb{R}$, $\\left|\\left|\\lambda{\\bf x}\\right|\\right|_{2} = \\left|\\lambda\\right| \\left|\\left|{\\bf x}\\right|\\right|_{2}$.\n",
    "* If we define $\\hat{{\\bf x}} = {\\bf x}/\\left|\\left|{\\bf x}\\right|\\right|_{2}$, then $\\left|\\left|\\hat{{\\bf x}}\\right|\\right|_{2} = 1 $.\n",
    "\n",
    "We have some definitions to keep in mind about different types of matrices.  \n",
    "\n",
    "* If $m=n$, we say the matrix $A$ is square.  \n",
    "* If $m<n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _underdetermined_, i.e. there are fewer equations than there are unknowns.  \n",
    "* If $m>n$, we say the matrix problem $A{\\bf x}={\\bf b}$ is _overdetermined_, i.e. there are more equations than there are unknowns.  \n",
    "* For $m\\times n$ matrix $A$, we define its _transpose, $A^{T}$ to be the $n \\times m$ matrix with entries $A^{T}_{ij} = A_{ji}$.  Visually, we see that $A^{T}$ is flipped relative to $A$, i.e. \n",
    "$$\n",
    "A^{T} = \\begin{pmatrix} A_{11} & A_{21} & \\cdots & A_{m1} \\\\ A_{12} & A_{22} & \\cdots & A_{m2}\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ A_{1n} & A_{2n} & \\cdots & A_{mn} \\end{pmatrix},~ A^{T}:\\mathbb{R}^{m}\\rightarrow \\mathbb{R}^{n}\n",
    "$$\n",
    "* Using the rules of matrix multiplication, if $A$ is an $m\\times n$ matrix, then $A^{T}A$ is an $n \\times n$ matrix and $AA^{T}$ is a $m \\times m$ matrix.  Thus for an overdetermined problem, we can transform it into a square problem of the form \n",
    "$$\n",
    "A^{T}A {\\bf x} = A^{T}{\\bf b}.\n",
    "$$\n",
    "\n",
    "Underdetermined problems are a bit messier to cope with.  To do so, we need to get more comfortable with norms.  To that end then:\n",
    "\n",
    "_Problems_: Show \n",
    "* $\\left|\\left|{\\bf x}\\right|\\right|^{2}_{2} = {\\bf x}^{T}{\\bf x}$\n",
    "* Show $(A^{T}A)^{T} = A^{T}A$ and $(AA^{T})^{T} = AA^{T}$.  Matrices which are equal to their own transpose are called _symmetric_ or _self-adjoint_.\n",
    "* Suppose $A^{T}{\\bf y} = 0$.  Show that if a solution to $A{\\bf x}={\\bf b}$ exists, then $\\left<{\\bf b},{\\bf y}\\right>=0$.  \n",
    "* For ${\\bf x},{\\bf y} \\in \\mathbb{R}^{n}$, show the _Cauchy-Schwarz_ inequality \n",
    "$$\n",
    "\\left|\\left<{\\bf x},{\\bf y}\\right>\\right| = \\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\left(\\sum_{j=1}^{n}|x_{j}|^{2}\\right)^{1/2}\\left(\\sum_{j=1}^{n}|y_{j}|^{2}\\right)^{1/2} = \\left|\\left|{\\bf x}\\right|\\right|_{2}\\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "Note, the triangle inequality for real numbers, $|a+b|\\leq |a|+|b|$ gives us via an inductive argument that \n",
    "$$\n",
    "\\left|\\sum_{j=1}^{n}x_{j}y_{j}\\right| \\leq \\sum_{j=1}^{n}\\left|x_{j}\\right|\\left|y_{j}\\right|\n",
    "$$\n",
    "* Using the Cauchy-Schwarz inequality and the triangle inequality, where $|a+b|\\leq |a|+|b|$, show \n",
    "$$\n",
    "\\left|\\left|{\\bf x}+{\\bf y}\\right|\\right|_{2} \\leq \\left|\\left|{\\bf x}\\right|\\right|_{2} + \\left|\\left|{\\bf y}\\right|\\right|_{2}.\n",
    "$$\n",
    "\n",
    "### Eigenvalues and Symmetric Matrices\n",
    "\n",
    "We say that a constant $\\lambda \\in \\mathbb{C}$ is an _eigenvalue_ of a square matrix $A$ if there exists some vector ${\\bf v}$ such that \n",
    "$$\n",
    "A{\\bf v} = \\lambda {\\bf v}.\n",
    "$$\n",
    "We call ${\\bf v}$ an _eigenvector_.  Note, the above equation is equivalent to \n",
    "$$\n",
    "(A - \\lambda I){\\bf v} = 0,\n",
    "$$\n",
    "which is equivalent to requiring that $\\mbox{det}(A-\\lambda I)=0$.  Note, we are usually obliged to worry about complex eigenvalues and eigenvectors for even real matrices.  \n",
    "\n",
    "_Problem_: For \n",
    "$$\n",
    "A = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0\\end{pmatrix},\n",
    "$$\n",
    "find its eigenvalues and eigenvectors.  \n",
    "\n",
    "However, we are only going to work with symmetric matrices where $A^{T}=A$.  In this case, one can show that $\\lambda$ and its corresponding ${\\bf v}$ are real.  This essentially comes from the formula\n",
    "$$\n",
    "\\lambda \\left|\\left|{\\bf v}\\right|\\right|^{2} = \\left<A{\\bf v},{\\bf v} \\right> = \\left<{\\bf v},A{\\bf v} \\right>.\n",
    "$$\n",
    "Thus, we get for symmetric matrices the formula for the eigenvalues $\\lambda$\n",
    "$$\n",
    "\\lambda = \\left<A\\hat{{\\bf v}},\\hat{{\\bf v}} \\right>, ~ \\hat{{\\bf v}} = {\\bf v}/ \\left|\\left|{\\bf v}\\right|\\right|.\n",
    "$$\n",
    "\n",
    "_Problem_: Show \n",
    "* For symmetric matrix $A$ that if it has two unequal eigenvalues, say $\\lambda_{1}$ and $\\lambda_{2}$, with \n",
    "$$\n",
    "A{\\bf v}_{1} = \\lambda_{1}{\\bf v}_{1}, ~ A{\\bf v}_{2} = \\lambda_{2}{\\bf v}_{2}\n",
    "$$\n",
    "that $\\left<{\\bf v}_{1},{\\bf v}_{2} \\right>=0$.\n",
    "* Suppose that for a symmetric matrix $A$, we find two eigenvectors, say ${\\bf v}_{1}$ and ${\\bf v}_{2}$ for the same eigenvalue $\\lambda$.  Show that one can find two eigenvectors $\\hat{\\bf v}_{1}$ and $\\hat{\\bf v}_{2}$ such that \n",
    "$$\n",
    "A\\hat{\\bf v}_{j} = \\lambda \\hat{\\bf v}_{j}, ~ \\left<\\hat{{\\bf v}}_{1}, \\hat{{\\bf v}}_{2}\\right>=0.\n",
    "$$\n",
    "* Suppose one has found $j-1$ eigenvalues and eigenvectors, say $\\lambda_{l}$ and $\\hat{{\\bf v}}_{l}$, $l=1,\\cdots,j-1$, of the matrix $A$, and $\\left<\\hat{{\\bf v}}_{l},\\hat{{\\bf v}}_{k} \\right>=\\delta_{lk}$.  Note, we do not require that the values $\\lambda_{l}$ all be distinct, i.e. eigenvalues can repeat.  Suppose one has a unit vector $\\hat{{\\bf x}}$ such that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\hat{{\\bf v}}_{l} \\right> = 0.\n",
    "$$\n",
    "Show that \n",
    "$$\n",
    "\\left<\\hat{{\\bf x}},\\sum_{l=1}^{j-1}c_{l}\\hat{{\\bf v}}_{l} \\right>  = 0, ~ c_{l} \\in \\mathbb{R}.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
