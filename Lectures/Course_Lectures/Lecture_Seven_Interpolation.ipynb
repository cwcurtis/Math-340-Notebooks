{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "\n",
    "So at this point, we now want to look at the following problem.  Suppose I give you some data in the form of a set of points\n",
    "\n",
    "$$\n",
    "\\left\\{x_{j},f_{j} \\right\\}_{j=0}^{n}\n",
    "$$\n",
    "\n",
    "where we think that $f_{j} = f(x_{j})$, which is to say, we think the data comes from a function $f(x)$, but we do not know the function $f(x)$.  Note, each point $x_{j}$ is called a _node_.  The question becomes, how might we find an approximation to $f(x)$?  As it turns out, there are an infinite number of ways to solve this problem, each with good and bad features.  The approach we will study first is called _Lagrange Interpolation_.  \n",
    "\n",
    "This method starts by deciding we are going to fit the data with an $n^{th}$ order polynomial, i.e. we choose a polynomial $P_{n}(x)$ of the form \n",
    "\n",
    "$$\n",
    "P_{n}(x) = p_{0} + p_{1}x + \\cdots + p_{n}x^{n},\n",
    "$$\n",
    "\n",
    "where the coefficients $p_{j}$ are found from the _interpolation formulas_\n",
    "\n",
    "$$\n",
    "P_{n}(x_{j}) = f_{j}, ~ j=0,\\cdots,n.\n",
    "$$\n",
    "\n",
    "As you can see, we have $n+1$ unknown coefficients $p_{j}$ and we have $n+1$ equations provided by the interpolation formulas.  Thus, we see that in principle we should be able to determine $P_{n}(x)$.  The advantage of having $P_{n}(x)$ is that anything else we want to know about $f(x)$, such as $f'(x)$ or $\\int f(x)dx$, we can find by using $P_{n}(x)$.  \n",
    "\n",
    "Now, there is another important way to think about $P_{n}(x)$.  While it is _completely equivalent_ to what we have described above, it forms a very fundamental way to think about interpolation that comes up again and again in applied mathematics and numerical analysis.  In this approach, we write $P_{n}(x)$ as \n",
    "\n",
    "$$\n",
    "P_{n}(x) = \\sum_{j=0}^{n} f_{j}L_{j}^{(n)}(x),\n",
    "$$\n",
    "\n",
    "where the functions $L_{j}^{(n)}(x)$ are themselves $n^{th}$-order polynomials which are defined so that \n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x_{j}) = 1, ~ L_{j}^{(n)}(x_{k}) = 0, ~k\\neq j. \n",
    "$$\n",
    "\n",
    "We can see this idea illustrated in the figure below.  Here, we are interpolating through the data set\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|r}\n",
    "x_{j} & f_{j}\\\\\n",
    "\\hline\n",
    "-9 & 5\\\\\n",
    "-4 & 2\\\\\n",
    "-1 & -2\\\\\n",
    "7 & 9\n",
    "\\end{array}\n",
    "$$\n",
    "![linterp](https://upload.wikimedia.org/wikipedia/commons/5/5a/Lagrange_polynomial.svg)\n",
    "\n",
    "So, if we think about it, we ultimately see that we can find the $L^{(n)}_{j}(x)$ via the formula\n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x) = \\frac{\\prod_{l\\neq j}^{n}(x-x_{l})}{\\prod_{l\\neq j}^{n}(x_{j}-x_{l})}.\n",
    "$$\n",
    "\n",
    "We can motivate this result by looking at simple cases and working our way up.  So, suppose we let $n=1$.  In this case, we need to find two linear functions $L^{(1)}_{0}(x)$ and $L^{(1)}_{1}(x)$.  As we can readily see, in order to satisfy our requirements for these functions, we can just use\n",
    "\n",
    "$$\n",
    "L^{(1)}_{0}(x) = \\frac{x-x_{1}}{x_{0}-x_{1}}, ~ L^{(1)}_{1}(x) = \\frac{x-x_{0}}{x_{1}-x_{0}}.\n",
    "$$\n",
    "\n",
    "We clearly see in this case that \n",
    "$$\n",
    "L^{(1)}_{0}(x_{0}) = 1, ~ L^{(1)}_{0}(x_{1}) = 0, ~~L^{(1)}_{1}(x_{0}) = 0, ~ L^{(1)}_{1}(x_{1}) = 1.\n",
    "$$\n",
    "\n",
    "Likewise, if we go to $n=2$, we need three different quadratic functions which, using either the formula or our intuition, we realize are \n",
    "\n",
    "$$\n",
    "L^{(2)}_{0}(x) = \\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}, ~ L^{(2)}_{1}(x) = \\frac{(x-x_{0})(x-x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})}, ~ L^{(2)}_{2}(x) = \\frac{(x-x_{0})(x-x_{1})}{(x_{2}-x_{1})(x_{2}-x_{1})}.\n",
    "$$\n",
    "\n",
    "So as we see, we can build $P_{n}(x)$ from the weighted $L^{(n)}_{j}(x)$ functions, which act as a _ basis _ for our interpolating polynomial.  The question then is, how can we numerically determine the functions $L^{(n)}_{j}(x)$.  This is done in the code below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfun(xvals,jj,x):\n",
    "    lval = np.ones(x.size)\n",
    "    xjj = xvals[jj]\n",
    "    xrem = xvals[:jj]\n",
    "    xrem = np.append(xrem,xvals[jj+1:])\n",
    "    denom = np.prod(xjj - xrem)\n",
    "    for val in xrem:\n",
    "        lval *= (x-val)\n",
    "    return lval/denom\n",
    "    return lval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagran_interp(xvals,fvals,x):\n",
    "    n = fvals.size\n",
    "    ipoly = np.zeros(x.size)\n",
    "    for jj in range(n):\n",
    "        ipoly += fvals[jj]*lfun(xvals,jj,x)\n",
    "    return ipoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to recreate the example seen in the figure above, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.array([-9.,-4.,-1.,7.])\n",
    "fvals = np.array([5.,2.,-2.,9.])\n",
    "xinterp = np.linspace(-9.,7.,int(1e2))\n",
    "ivals = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,ivals,color='k')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$P_{3}(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that we have recreated said example exactly.  Okay, good, we have working code.  However, we are now going to look at a classic example of what is called the _Runge Phenomena_, which in effect shows us that we cannot really go all that far with Lagrange interpolation if we insist on using equally spaced points. \n",
    "\n",
    "To see this, we let \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+x^{2}}, ~ -1\\leq x \\leq 1.\n",
    "$$\n",
    "\n",
    "Now keep in mind, the more nodes I use in my interpolation scheme, the higher the degree polynomial I am obliged to use.  So suppose I use relatively few nodes, like only 10.  Then I get the following error plot for Lagrange interpolation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xinterp = np.linspace(-1.,1.,int(1e3)+1)\n",
    "ftrue = 1./(1.+xinterp**2.)\n",
    "xvals = xinterp[::100]\n",
    "fvals = ftrue[::100]\n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we double the number of points, aside from the fact that the error is several orders of magnitude larger at the endpoints, overall, everything improves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xinterp[::50]\n",
    "fvals = ftrue[::50]\n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at this point, we might imagine that more nodes means greater accuracy.  And that is in some sense true.  And if we double the number of points again, we get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xinterp[::25]\n",
    "fvals = ftrue[::25]\n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So again, endpoints are problematic, but at least away from the endpoints, everything improved, and at least nothing got worse at either boundary.  So if we keep adding nodes, then we should eventually be able to get to machine precision, right? Wrong.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = xinterp[::12]\n",
    "fvals = ftrue[::12]\n",
    "finterp = lagran_interp(xvals,fvals,xinterp)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, more nodes actually made our approxmiation _worse_, not better.  Further, as we see, while we can achieve machine precision on the interior of our interpolation scheme, we cannot get anywhere close to that at the boundaries.  In order to get around this then, we might think of a different way to position nodes to address this issue.  Thus, we get motivate discussing clustered meshes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Meshes\n",
    "\n",
    "So, a way to get around the Runge Phenomena is to use unevenly spaced meshes of points.  To wit, we use what are called the Chebyshev points or nodes, which are given by \n",
    "\n",
    "$$\n",
    "x_{j} = \\cos\\left(\\frac{2j+1}{2n+2}\\pi\\right), ~ j=0,\\cdots,n\n",
    "$$\n",
    "\n",
    "As we see below, by essentially clustering nodes at the endpoints of the interval we wish to interpolate over, we can remove the Runge Phenomena.  This incidentally is the beginning of a long conversation in numerical analysis we will not pursue further here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncheb = 40\n",
    "xcheb = np.cos(np.pi*(2.*np.arange(ncheb+1)+1.)/(2.*ncheb+2.))\n",
    "fcheb = 1./(1.+xcheb**2.)\n",
    "finterp = lagran_interp(xcheb,fcheb,xinterp)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Splines\n",
    "\n",
    "Now we are going to try a very different approach to function approximation compared to what we did above with Lagrange Interpolation, though identical to how we developed the Trapezoid Method.  Instead of one global interpolating polynomial for given data $\\left\\{(x_{j},f_{j})\\right\\}^{N}_{j=0}$, we are going to use a piecewise defined function $S(x)$ where\n",
    "\n",
    "$$\n",
    "S(x) = \\left\\{ \\begin{array}{rl} S_{0}(x) & x_{0}\\leq x < x_{1} \\\\ S_{1}(x) & x_{1}\\leq x < x_{2} \\\\ \\vdots & \\vdots \\\\ S_{n-1}(x) & x_{n-1}\\leq x \\leq x_{n} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "with the interpolation requirements\n",
    "\n",
    "$$\n",
    "S_{j}(x_{j}) = f_{j}, ~ j=0,\\cdots,n-1, ~ S_{n-1}(x_{n}) = f_{n},\n",
    "$$\n",
    "\n",
    "and the continuity requirements at the interior points $\\left\\{x_{j}\\right\\}^{n-1}_{j=1}$\n",
    "\n",
    "$$\n",
    "S_{j}(x_{j+1}) = S_{j+1}(x_{j+1}), ~ j=0,\\cdots,n-2.\n",
    "$$\n",
    "\n",
    "By choosing $S_{j}(x) = m_{j}(x-x_{j}) + f_{j}$, this continuity requirement gives us that \n",
    "\n",
    "$$\n",
    "m_{j}(x_{j+1}-x_{j}) + f_{j} = f_{j+1},\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "m_{j} = \\frac{f_{j+1}-f_{j}}{x_{j+1}-x_{j}}, ~ j=0,\\cdots,n-1.\n",
    "$$\n",
    "\n",
    "To implement this then, we use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_spline(xvals,fvals,xquery):\n",
    "    npts = xvals.size\n",
    "    qpts = xquery.size\n",
    "    svals = np.zeros(qpts)\n",
    "    # Precompute the slopes\n",
    "    mvals = (fvals[1:]-fvals[:(npts-1)])/(xvals[1:]-xvals[:(npts-1)])\n",
    "    # Now find the appropriate interval for each query point\n",
    "    for ll in range(qpts):\n",
    "        for jj in range(npts - 1):\n",
    "            if xvals[jj]<=xquery[ll] and xquery[ll]<xvals[jj+1]:\n",
    "                svals[ll] = mvals[jj]*(xquery[ll]-xvals[jj]) + fvals[jj]\n",
    "                break\n",
    "            svals[ll] = mvals[npts-2]*(xquery[ll]-xvals[npts-2]) + fvals[npts-2]\n",
    "\n",
    "    return svals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xquery = np.linspace(-1.,1.,int(1e3)+1)\n",
    "ftrue = 1./(1.+xquery**2.)\n",
    "xvals = xquery[::12]\n",
    "fvals = ftrue[::12]\n",
    "finterp = linear_spline(xvals,fvals,xquery)\n",
    "plt.plot(xinterp,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
