{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation\n",
    "\n",
    "So at this point, we now want to look at the following problem.  Suppose I give you some data in the form of a set of points\n",
    "\n",
    "$$\n",
    "\\left\\{(x_{j},f_{j}) \\right\\}_{j=0}^{n}\n",
    "$$\n",
    "\n",
    "where we think that $f_{j} = f(x_{j})$, which is to say, we think the data comes from a function $f(x)$, but we do not know the function $f(x)$.  Note, each point $x_{j}$ is called a _node_.  The question becomes, how might we find an approximation to $f(x)$?  As it turns out, there are an infinite number of ways to solve this problem, each with good and bad features.  The approach we will study first is called _Lagrange Interpolation_.  \n",
    "\n",
    "This method starts by deciding we are going to fit the data with an $n^{th}$ order polynomial, i.e. we choose a polynomial $P_{n}(x)$ of the form \n",
    "\n",
    "$$\n",
    "P_{n}(x) = p_{0} + p_{1}x + \\cdots + p_{n}x^{n},\n",
    "$$\n",
    "\n",
    "where the coefficients $p_{j}$ are found from the _interpolation formulas_\n",
    "\n",
    "$$\n",
    "P_{n}(x_{j}) = f_{j}, ~ j=0,\\cdots,n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Two-Nodes Case\n",
    "\n",
    "So, suppose we have interpolation data $(x_{0}, f_{0})$ and $(x_{1}, f_{1})$.  If we start from the general polynomial $p_{1}(x) = p_{0} + p_{1}x$, then if we use our interpolation requirements, we get the following system of equations\n",
    "\n",
    "\\begin{align*}\n",
    "p_{0} + p_{1}x_{0} = &  f_{0}\\\\\n",
    "p_{0} + p_{1}x_{1} = & f_{1}\n",
    "\\end{align*}\n",
    "\n",
    "which can be rewritten in the form \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 1 & x_{0} \\\\ 1 & x_{1}\\end{pmatrix} \\begin{pmatrix}p_{0} \\\\ p_{1} \\end{pmatrix} = \\begin{pmatrix}f_{0} \\\\ f_{1}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Inverting, we find the solution \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}p_{0} \\\\ p_{1} \\end{pmatrix} = \\frac{1}{x_{1}-x_{0}}\\begin{pmatrix} x_{1} & -x_{0} \\\\ -1 & 1\\end{pmatrix} \\begin{pmatrix}f_{0} \\\\ f_{1}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This then gets us the solution for $p_{1}(x)$ in the form \n",
    "\n",
    "$$\n",
    "p_{1}(x) = \\frac{f_{0}x_{1}-f_{1}x_{0}}{x_{1}-x_{0}} + \\frac{f_{1}-f_{0}}{x_{1}-x_{0}}x\n",
    "$$\n",
    "\n",
    "As we can see, if we rewrite this in the following form\n",
    "\n",
    "$$\n",
    "p_{1}(x) = f_{0}L^{(1)}_{0}(x) + f_{1}L^{(1)}_{1}(x), \n",
    "$$\n",
    "\n",
    "then we have that \n",
    "\n",
    "$$\n",
    "L^{(1)}_{0}(x) = \\frac{x-x_{1}}{x_{0}-x_{1}}, ~ L^{(1)}_{1}(x) = \\frac{x-x_{0}}{x_{1}-x_{0}}.\n",
    "$$\n",
    "\n",
    "We clearly see in this case that \n",
    "$$\n",
    "L^{(1)}_{0}(x_{0}) = 1, ~ L^{(1)}_{0}(x_{1}) = 0, ~~L^{(1)}_{1}(x_{0}) = 0, ~ L^{(1)}_{1}(x_{1}) = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Three-Nodes Case\n",
    "\n",
    "So now suppose that we have the interpolation data $\\left\\{(x_{j},f_{j})\\right\\}_{j=0}^{2}$.  Following the model above, to find $p_{2}(x)$ we have to turn the system of equations \n",
    "\n",
    "\\begin{align*}\n",
    "p_{0} + p_{1}x_{0} + p_{2}x_{0}^{2}= &  f_{0}\\\\\n",
    "p_{0} + p_{1}x_{1} + p_{2}x_{1}^{2}= & f_{1} \\\\\n",
    "p_{0} + p_{1}x_{2} + p_{2}x_{2}^{2}= & f_{2} \n",
    "\\end{align*}\n",
    "\n",
    "into the linear system\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 1 & x_{0} & x_{0}^{2} \\\\ 1 & x_{1} & x_{1}^{2} \\\\ 1 & x_{2} & x_{2}^{2} \\end{pmatrix} \\begin{pmatrix}p_{0} \\\\ p_{1}\\\\ p_{2} \\end{pmatrix} = \\begin{pmatrix}f_{0} \\\\ f_{1} \\\\ f_{2}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now, while we could find the inverse of the above matrix, it is not a pleasant process.  So instead, using the intuition we developed above, we look for $p_{2}(x)$ via the representation\n",
    "\n",
    "$$\n",
    "p_{2}(x) = \\sum_{j=0}^{2}f_{j}L^{(2)}_{j}(x), ~ L^{(2)}_{j}(x_{j}) = 1, ~ L^{(2)}_{j}(x_{k}) = 0 ~\\text{for} ~ j\\neq k.\n",
    "$$\n",
    "\n",
    "Using a bit of algebraic intuition, it is not too difficult to work out that \n",
    "\n",
    "$$\n",
    "L^{(2)}_{0}(x) = \\frac{(x-x_{1})(x-x_{2})}{(x_{0}-x_{1})(x_{0}-x_{2})}, ~ L^{(2)}_{1}(x) = \\frac{(x-x_{0})(x-x_{2})}{(x_{1}-x_{0})(x_{1}-x_{2})}, ~ L^{(2)}_{2}(x) = \\frac{(x-x_{0})(x-x_{1})}{(x_{2}-x_{1})(x_{2}-x_{1})}.\n",
    "$$\n",
    "\n",
    "So as we see, we can build $P_{n}(x)$ from the weighted $L^{(n)}_{j}(x)$ functions, which act as a _basis_ for our interpolating polynomial.  The question then is, how can we numerically determine the functions $L^{(n)}_{j}(x)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The General Case\n",
    "\n",
    "As you can see, we have $n+1$ unknown coefficients $p_{j}$ and we have $n+1$ equations provided by the interpolation formulas, i.e.\n",
    "\n",
    "\\begin{align*}\n",
    "p_{0} + p_{1}x_{0} + p_{2}x_{0}^{2} + \\cdots + p_{n}x_{0}^{n}= &  f_{0}\\\\\n",
    "p_{0} + p_{1}x_{1} + p_{2}x_{1}^{2} + \\cdots + p_{n}x_{1}^{n}= & f_{1} \\\\\n",
    "\\vdots \\\\\n",
    "p_{0} + p_{1}x_{n} + p_{2}x_{n}^{2} + \\cdots + p_{n}x_{n}^{n} = & f_{n} \n",
    "\\end{align*}\n",
    "\n",
    "which can be rewritten as a matrix/vector problem in the form\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{n} \\begin{pmatrix}p_{0} \\\\ p_{1}\\\\ \\vdots \\\\ p_{n} \\end{pmatrix} = \\begin{pmatrix}f_{0} \\\\ f_{1} \\\\ \\vdots \\\\ f_{n}\\end{pmatrix}, ~ \\mathbf{V}_{n} = \\begin{pmatrix} 1 & x_{0} & x_{0}^{2} & \\cdots & x_{0}^{n} \\\\ 1 & x_{1} & x_{1}^{2} & \\cdots & x_{1}^{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{n} & x_{n}^{2} & \\cdots & x_{n}^{n} \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note, the $(n+1)\\times (n+1)$ matrix $\\mathbf{V}_{n}$ is called a _Van der Monde_ matrix.  Thus, we see that in principle we should be able to determine $P_{n}(x)$.  The advantage of having $P_{n}(x)$ is that anything else we want to know about $f(x)$, such as $f'(x)$ or $\\int f(x)dx$, we can find by using $P_{n}(x)$.  \n",
    "\n",
    "Now, working directly with the above linear system is costly and computationally difficult.  So while it is _completely equivalent_ to what we have described above, we gain a massive advantage when we write $P_{n}(x)$ as \n",
    "\n",
    "$$\n",
    "P_{n}(x) = \\sum_{j=0}^{n} f_{j}L_{j}^{(n)}(x),\n",
    "$$\n",
    "\n",
    "where the functions $L_{j}^{(n)}(x)$ are themselves $n^{th}$-order polynomials which are defined so that \n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x_{j}) = 1, ~ L_{j}^{(n)}(x_{k}) = 0, ~k\\neq j. \n",
    "$$\n",
    "\n",
    "We can see this idea illustrated in the figure below.  Here, we are interpolating through the data set\n",
    "\n",
    "$$\n",
    "\\begin{array}{r|r}\n",
    "x_{j} & f_{j}\\\\\n",
    "\\hline\n",
    "-9 & 5\\\\\n",
    "-4 & 2\\\\\n",
    "-1 & -2\\\\\n",
    "7 & 9\n",
    "\\end{array}\n",
    "$$\n",
    "![linterp](https://upload.wikimedia.org/wikipedia/commons/5/5a/Lagrange_polynomial.svg)\n",
    "\n",
    "So, if we think about it, using the Fundamental Theorem of Algebra, which says that we should be able to factor the $n^{th}$ degree polynomial $L^{(n)}_{j}(x)$ by its $n$-roots $\\left\\{x_{k}\\right\\}_{k\\neq j}$, then we must have\n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x) = c_{j}\\prod_{l\\neq j}^{n}(x-x_{l}).\n",
    "$$\n",
    "\n",
    "To find the _normalizing coefficient_ $c_{j}$, we note that from the requirement that $L^{(n)}_{j}(x_{j})=1$, then \n",
    "\n",
    "$$\n",
    "1 = c_{j}\\prod_{l\\neq j}^{n}(x_{j}-x_{l}),\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "L_{j}^{(n)}(x) = \\frac{\\prod_{l\\neq j}^{n}(x-x_{l})}{\\prod_{l\\neq j}^{n}(x_{j}-x_{l})}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds L_j(x)\n",
    "# Inputs: node points (I also call them mesh points) xnodes, index jj, and query point xquery\n",
    "\n",
    "def lfun(xnodes,jj,xquery):\n",
    "    lvals = np.ones(xquery.size)\n",
    "    # Find the j^th node\n",
    "    xnodesjj = xnodes[jj]\n",
    "    # We need to build a list of node points which does not include xjj\n",
    "    xnodesrem = xnodes[:jj]\n",
    "    xnodesrem = np.append(xnodesrem,xnodes[(jj+1):])\n",
    "    denominator = np.prod(xnodesjj-xnodesrem)\n",
    "    for val in xnodesrem:\n",
    "        lvals *= (xquery-val)\n",
    "    return lvals/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds p_n(x)\n",
    "# Inputs: data points xnodes and fvals, and query point x\n",
    "\n",
    "def lagrange_interpolator(xnodes,fvals,xquery):\n",
    "    n = fvals.size\n",
    "    ipoly = np.zeros(xquery.size)\n",
    "    for jj in range(n):\n",
    "        ipoly += fvals[jj]*lfun(xnodes,jj,xquery)\n",
    "    return ipoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our code, we use the function \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1+x^{2}}, ~ -1\\leq x \\leq 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftest = lambda x: 1./(1.+x**2.)\n",
    "xquery = np.linspace(-1.,1.,int(1e3)+1) # we build an array of 1001 equally spaced query points (so 1000 intervals).  \n",
    "ftrue = ftest(xquery) # this generates our \"true\" function values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnodes = np.linspace(-1.,1.,41)\n",
    "fvals = ftest(xnodes)\n",
    "finterp = lagrange_interpolator(xnodes,fvals,xquery)\n",
    "plt.plot(xquery,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnodes = np.linspace(-1.,1.,81)\n",
    "fvals = ftest(xnodes)\n",
    "finterp = lagrange_interpolator(xnodes,fvals,xquery)\n",
    "plt.plot(xquery,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Meshes\n",
    "\n",
    "So, a way to get around the Runge Phenomena is to use unevenly spaced meshes of points.  To wit, we use what are called the Chebyshev points or nodes, which are given by \n",
    "\n",
    "$$\n",
    "x_{j} = \\cos\\left(\\frac{2j+1}{2n+2}\\pi\\right), ~ j=0,\\cdots,n\n",
    "$$\n",
    "\n",
    "As we see below, by essentially clustering nodes at the endpoints of the interval we wish to interpolate over, we can remove the Runge Phenomena.  This incidentally is the beginning of a long conversation in numerical analysis we will not pursue further here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncheb = 40\n",
    "xcheb = np.cos(np.pi*(2.*np.arange(ncheb+1)+1.)/(2.*ncheb+2.))\n",
    "fcheb = 1./(1.+xcheb**2.)\n",
    "finterp = lagrange_interpolator(xcheb,fcheb,xquery)\n",
    "plt.plot(xquery,np.ma.log10(np.abs(ftrue-finterp)),ls='-',color='k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
